---
title: "Modeling"
format: html
---

```{r}
#| include: false

# Make data available for modeling (will not be displayed in rendered page)

# libraries
library(tidyverse)
library(tidymodels)

# read in data file 
diabetes <- read_csv(file = "diabetes_binary_health_indicators_BRFSS2015.csv")

# clean up data
diabetes <- diabetes |>
  mutate(Diabetes_binary = as.factor(Diabetes_binary),
         HighBP = as.factor(HighBP),
         HighChol = as.factor(HighChol),
         CholCheck = as.factor(CholCheck),
         Smoker = as.factor(Smoker),
         Stroke = as.factor(Stroke),
         HeartDiseaseorAttack = as.factor(HeartDiseaseorAttack),
         PhysActivity = as.factor(PhysActivity),
         Fruits = as.factor(Fruits),
         Veggies = as.factor(Veggies),
         HvyAlcoholConsump = as.factor(HvyAlcoholConsump),
         AnyHealthcare = as.factor(AnyHealthcare),
         NoDocbcCost = as.factor(NoDocbcCost),
         DiffWalk = as.factor(DiffWalk),
         GenHlth = factor(GenHlth,
                          levels = 1:5,
                          labels = c("Excellent", "Very Good", "Good", "Fair", "Poor")),
         Sex = factor(Sex, levels = 0:1, labels = c("Female", "Male")),
         Age = factor(Age,
                      levels = 1:13,
                      labels = c("18 to 24", "25 to 29", "30 to 34", "35 to 39", "40 to 44", 
                                 "45 to 49", "50 to 54", "55 to 59", "60 to 64", "65 to 69", 
                                 "70 to 74", "75 to 79", "80 and over")),
         Education = factor(Education,
                            levels = 1:6,
                            labels = c("Never attended/Only Kindergarten", "Elementary", 
                                       "Some high school", "High school graduate", 
                                       "Some college/tech school", "College graduate")),
         Income = factor(Income,
                         levels = 1:8,
                         labels = c("Less than $10,000", "$10,000 to $14,999", "$15,000 to $19,999", 
                                    "$20,000 to $24,999", "$25,000 to $34,999", "$35,000 to $49,999",
                                    "$50,000 to $74,999", "$75,000 or greater"))
         )

```

## Introduction

Using a subset of variables from the [Diabetes Health Indicators Dataset](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/), two types of tree-based models will now be trained on a training set using 5-fold CV. Training data will be used to assess the log-loss of a classification tree model and a random forest model. The best parameters from each will be selected and then fit on the test set to determine which model is best. That final model will then be fit to the entire dataset, and an API will be set up with three endpoints. The first endpoint will return a prediction of diabetes status based on the input parameters; the second endpoint will provide my name and the github URL; and the third will show a plot of the confusion matrix for the final model.

## Setup

Data needs to be split, with 70% training and 30% testing

```{r}
set.seed(42)

diabetes_split <- initial_split(diabetes, prop = 0.7)
diabetes_train <- training(diabetes_split)
diabetes_test <- testing(diabetes_split)
```

Set up CV folds

```{r}
diabetes_CV_folds <- vfold_cv(diabetes_train, 5)
```

### Recipe

Numeric predictors will be standardized, and dummy variables will be created for the binary and categorial variables.

```{r}
tree_rec <- recipe(Diabetes_binary ~ GenHlth + HighBP + HighChol + DiffWalk + 
                     BMI + PhysHlth,
                   data = diabetes_train) |>
  step_normalize(all_numeric()) |>
  step_dummy(GenHlth, HighBP, HighChol, DiffWalk)

tree_rec |>
  prep(diabetes_train) |>
  bake(diabetes_train)
```

## Classification Tree

A classification tree is a supervised learning model that attempts to predict the value of a categorical response variable based on the values of one or more predictor variables. It does this by partitioning the predictor space into different branches, ultimately arriving at a prediction for the response variable. These branches are created using recursive binary splitting, which determines the optimal split for the current node only. The best split is determined by minimizing the squared error loss evaluated for each potential value of the predictors. Since it does not consider potential future splits, this can result in the tree missing out on a better match down the line that could have led to more accurate predictions. However, the classification tree is easy to understand, and is an effective and flexible model for situations where the data does not conform well to a line or plane.  

### Set up Model and Workflow

Model for classification tree, with tuning parameters

```{r}
tree_mod <- decision_tree(tree_depth = tune(),
                          min_n = 10,
                          cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")
```

Workflow

```{r}
tree_wkf <- workflow() |>
  add_recipe(tree_rec) |>
  add_model(tree_mod)
```

### Tuning parameters

Set up a tuning grid

```{r}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = c(10, 5))
```

Run tuning on the specified grid

```{r}
#| label: tune-grid
#| cache: true

tree_fits <- tree_wkf |>
  tune_grid(resamples = diabetes_CV_folds,
            grid = tree_grid,
            metrics = metric_set(accuracy, mn_log_loss))
```

Get the best parameters

```{r}
tree_best_params <- select_best(tree_fits, metric = "mn_log_loss")

tree_best_params
```

Refit on the training set

```{r}
tree_final_fit <- tree_wkf |>
  finalize_workflow(tree_best_params) |>
  last_fit(diabetes_split, metrics = metric_set(accuracy, mn_log_loss))

tree_final_fit |>
  collect_metrics()
```

### Final Model

```{r}
tree_final_model <- extract_workflow(tree_final_fit)

tree_final_model |>
  extract_fit_engine() |>
  rpart.plot::rpart.plot(roundint = FALSE)
```

## Random Forest

A random forest model is a supervised learning ensemble method that utilizes bootstrapping to generate many single trees to predict a response variable based on the values of one or more predictor variables. Bootstrapping is a method of resampling the data with replacement; in a random forest model, those samples are then used to fit many trees, and the best prediction across all trees is chosen. For regression trees, the best prediction is the average of all predictions, and for classification trees, the best prediction is the most common one. In a random forest model, only a random subset of predictors is considered for each split of a single tree. This makes the different trees less correlated with each other, which has the potential to improve the overall accuracy of the prediction by reducing the variance between the predictions of the single trees.

### Set up Model and Workflow

```{r}
# model specifications
rf_model <- rand_forest(mtry = tune()) |>
  set_engine("ranger") |>
  set_mode("classification")

# workflow using same recipe already set up
rf_wkf <- workflow() |>
  add_recipe(tree_rec) |>
  add_model(rf_model)
```

### Tuning parameters

Tune on a grid with 10 values

```{r}
#| label: rf-tune-grid
#| cache: true

rf_fit <- rf_wkf |>
  tune_grid(resamples = diabetes_CV_folds,
            grid = 10,
            metrics = metric_set(accuracy, mn_log_loss))
```

Select best parameters

```{r}
rf_best_params <- rf_fit |>
  select_best(metric = "mn_log_loss")

rf_best_params
```

### Final Model

Finalize the workflow

```{r}
rf_final_fit <- rf_wkf |>
  finalize_workflow(rf_best_params) |>
  last_fit(diabetes_split, metrics = metric_set(accuracy, mn_log_loss))

```


## Compare Models

Compare the log loss for both models

```{r}
rf_metrics <- rf_final_fit |>
  collect_metrics() |>
  bind_cols(method = rep("Random Forest", 2)) 

tree_metrics <- tree_final_fit |>
  collect_metrics() |>
  bind_cols(method = rep("Classification Tree", 2)) 

rf_metrics |>
  bind_rows(tree_metrics)
```

The accuracy of each model is roughly the same, but Random Forest has a marginally better log loss, so this one is the overall winner.

## Fit Best Model to Entire Data

Fit the best model to the entire data set, for use by the API.

```{r}
#| eval: false

# model specifications (using the optimal mtry value found during tuning)
rf_model <- rand_forest(mtry = 4) |>
  set_engine("ranger") |>
  set_mode("classification")

# workflow 
rf_wkf <- workflow() |>
  add_recipe(tree_rec) |>
  add_model(rf_model)

# fit best model on entire data set
rf_fit_full <- rf_wkf |>
  fit(diabetes)

# save model for later use
saveRDS(rf_fit_full, file = "rf_fit_model.rda")

```

Also saved the confusion matrix, since it's a tad long-running

```{r}
#| eval: false

# save the confusion matrix
rf_conf_mat <- conf_mat(diabetes |> mutate(estimate = rf_fit_model |> 
                                             predict(diabetes) |> pull()),
                        Diabetes_binary,
                        estimate)

saveRDS(rf_conf_mat, file = "rf_conf_matrix.rda")
```


