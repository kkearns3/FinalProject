[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "This project utilizes the Diabetes Health Indicators Dataset, which was sourced from the CDC’s Behavioral Risk Factor Surveillance System survey from the year 2015. The dataset contains a collection of health indicators, along with a binary field indicating diabetes status; a value of 1 means the respondent has diabetes or prediabetes, and a value of 0 means he or she does not. The EDA in this file was performed with the goal of identifying a subset of variables that can be used as predictors for two tree-based models. I end up choosing two numeric predictors, BMI and PhysHlth. BMI is body mass index, and PhysHlth is an integer between 0 and 30 that indicates how many days in the last 30 days that the respondent experienced illness or injury. I choose one categorical predictor, GenHlth, in which the respondent rates their own general health on a scale from 1 for excellent through 5 for poor. The final three predictors I choose are binary, with values of 1 or 0 indicating the presence or absence (respectively) of the specified condition. These variables are HighBP (high blood pressure), HighChol (high cholesterol), and DiffWalk (difficulty walking)."
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "EDA",
    "section": "",
    "text": "This project utilizes the Diabetes Health Indicators Dataset, which was sourced from the CDC’s Behavioral Risk Factor Surveillance System survey from the year 2015. The dataset contains a collection of health indicators, along with a binary field indicating diabetes status; a value of 1 means the respondent has diabetes or prediabetes, and a value of 0 means he or she does not. The EDA in this file was performed with the goal of identifying a subset of variables that can be used as predictors for two tree-based models. I end up choosing two numeric predictors, BMI and PhysHlth. BMI is body mass index, and PhysHlth is an integer between 0 and 30 that indicates how many days in the last 30 days that the respondent experienced illness or injury. I choose one categorical predictor, GenHlth, in which the respondent rates their own general health on a scale from 1 for excellent through 5 for poor. The final three predictors I choose are binary, with values of 1 or 0 indicating the presence or absence (respectively) of the specified condition. These variables are HighBP (high blood pressure), HighChol (high cholesterol), and DiffWalk (difficulty walking)."
  },
  {
    "objectID": "EDA.html#data",
    "href": "EDA.html#data",
    "title": "EDA",
    "section": "Data",
    "text": "Data\nRead in file\n\ndiabetes &lt;- read_csv(file = \"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndiabetes\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\nAccording to the documentation of the dataset in kaggle:\n\nDiabetes_binary: 0 = no diabetes, 1 = prediabetes or diabetes\nBinary fields indicating 1 for presence or 0 for absence of the specified condition\n\nHighBP, HighChol, Stroke, HeartDiseaseorAttack\nDiffWalk: difficulty walking or climbing stairs\n\nBinary fields that need additional description to understand what it is representing\n\nCholCheck: Cholesterol check in 5 years?\nSmoker: Have you smoked at least 100 cigarettes in your entire lifetime?\nPhysActivity: physical activity in the past 30 days - not including job\nFruits: Consume fruit 1 or more times per day\nVeggies: Consume veggies 1 or more times per day\nHeavyAlcoholConsump: Adult men &gt;= 14 drinks per week, Adult women &gt;= 7 drinks per week\nAnyHealthcare: Has any kind of healthcare/insurance coverage\nNoDocbcCost: Any time in the last 12 months respondent needed to see a doctor, but did not due to cost?\n\nNumeric fields\n\nBMI\nMentHlth: days of poor mental health in the last 30 days\nPhysHlth: days of physical illness or injury in the last 30 days\n\nNumeric fields, but represent levels. Documentation provided a link to the CDC codebook here.\n\nGenHlth: self-rating of personal health from 1 = excellent to 5 = poor\nAge: 1 = 18-24, 2 = 25-29, 3 = 30-34, 4 = 35-39, 5 = 40-44, 6 = 45-49, 7 = 50-54, 8 = 55-59, 9 = 60-64, 10 = 65-69, 11 = 70-74, 12 = 75-79, 13 = 80+\nEducation: 1 = Never attended, or only kindergarten, 2 = Grades 1-8 (Elementary), 3 = Grades 9-11 (some high school), 4 = Grades 12 or GED (High school graduate), 5 = College 1-3 years (Some college or technical school), 6 = College 4 years or more (College graduate)\nIncome: 1 = Less than $10,000, 2 = $10,000-14,999, 3 = $15,000-19,999, 4 = $20,000-24,999, 5 = $25,000-34,999, 6 = $35,000-$49,999, 7 = $50,000-74,999, 8 = $75,000+\nSex: 0 = Female, 1 = Male\n\n\n\npsych::describe(diabetes)\n\n                     vars      n  mean   sd median trimmed  mad min max range\nDiabetes_binary         1 253680  0.14 0.35      0    0.05 0.00   0   1     1\nHighBP                  2 253680  0.43 0.49      0    0.41 0.00   0   1     1\nHighChol                3 253680  0.42 0.49      0    0.41 0.00   0   1     1\nCholCheck               4 253680  0.96 0.19      1    1.00 0.00   0   1     1\nBMI                     5 253680 28.38 6.61     27   27.68 4.45  12  98    86\nSmoker                  6 253680  0.44 0.50      0    0.43 0.00   0   1     1\nStroke                  7 253680  0.04 0.20      0    0.00 0.00   0   1     1\nHeartDiseaseorAttack    8 253680  0.09 0.29      0    0.00 0.00   0   1     1\nPhysActivity            9 253680  0.76 0.43      1    0.82 0.00   0   1     1\nFruits                 10 253680  0.63 0.48      1    0.67 0.00   0   1     1\nVeggies                11 253680  0.81 0.39      1    0.89 0.00   0   1     1\nHvyAlcoholConsump      12 253680  0.06 0.23      0    0.00 0.00   0   1     1\nAnyHealthcare          13 253680  0.95 0.22      1    1.00 0.00   0   1     1\nNoDocbcCost            14 253680  0.08 0.28      0    0.00 0.00   0   1     1\nGenHlth                15 253680  2.51 1.07      2    2.45 1.48   1   5     4\nMentHlth               16 253680  3.18 7.41      0    1.04 0.00   0  30    30\nPhysHlth               17 253680  4.24 8.72      0    1.77 0.00   0  30    30\nDiffWalk               18 253680  0.17 0.37      0    0.09 0.00   0   1     1\nSex                    19 253680  0.44 0.50      0    0.43 0.00   0   1     1\nAge                    20 253680  8.03 3.05      8    8.17 2.97   1  13    12\nEducation              21 253680  5.05 0.99      5    5.15 1.48   1   6     5\nIncome                 22 253680  6.05 2.07      7    6.35 1.48   1   8     7\n                      skew kurtosis   se\nDiabetes_binary       2.08     2.34 0.00\nHighBP                0.29    -1.92 0.00\nHighChol              0.31    -1.91 0.00\nCholCheck            -4.88    21.83 0.00\nBMI                   2.12    11.00 0.01\nSmoker                0.23    -1.95 0.00\nStroke                4.66    19.69 0.00\nHeartDiseaseorAttack  2.78     5.72 0.00\nPhysActivity         -1.20    -0.57 0.00\nFruits               -0.56    -1.69 0.00\nVeggies              -1.59     0.54 0.00\nHvyAlcoholConsump     3.85    12.85 0.00\nAnyHealthcare        -4.18    15.48 0.00\nNoDocbcCost           3.00     6.97 0.00\nGenHlth               0.42    -0.38 0.00\nMentHlth              2.72     6.44 0.01\nPhysHlth              2.21     3.50 0.02\nDiffWalk              1.77     1.15 0.00\nSex                   0.24    -1.94 0.00\nAge                  -0.36    -0.58 0.01\nEducation            -0.78     0.04 0.00\nIncome               -0.89    -0.28 0.00\n\n\n\nThe max number for BMI looks kind of surprising, at 98. Since BMI of greater than or equal to 30 is obese, I think it makes sense to look at the distribution of the values for this variable to see if it looks reasonable for a sample that should somewhat represent the general population. The median of 27 and relatively low standard deviation of 6.61 show it’s most likely the really high values are outliers, but it would be good to get a more thorough look at summary statistics.\nAll other values appear consistent with what the documentation describes. There are a lot of binary variables, and then there are a collection of variables with coded numeric values that need to be translated to a descriptive label.\n\n\nData Cleaning\nRecast data columns\n\ndiabetes &lt;- diabetes |&gt;\n  mutate(Diabetes_binary = as.factor(Diabetes_binary),\n         HighBP = as.factor(HighBP),\n         HighChol = as.factor(HighChol),\n         CholCheck = as.factor(CholCheck),\n         Smoker = as.factor(Smoker),\n         Stroke = as.factor(Stroke),\n         HeartDiseaseorAttack = as.factor(HeartDiseaseorAttack),\n         PhysActivity = as.factor(PhysActivity),\n         Fruits = as.factor(Fruits),\n         Veggies = as.factor(Veggies),\n         HvyAlcoholConsump = as.factor(HvyAlcoholConsump),\n         AnyHealthcare = as.factor(AnyHealthcare),\n         NoDocbcCost = as.factor(NoDocbcCost),\n         DiffWalk = as.factor(DiffWalk),\n         GenHlth = factor(GenHlth,\n                          levels = 1:5,\n                          labels = c(\"Excellent\", \"Very Good\", \"Good\", \"Fair\", \"Poor\")),\n         Sex = factor(Sex, levels = 0:1, labels = c(\"Female\", \"Male\")),\n         Age = factor(Age,\n                      levels = 1:13,\n                      labels = c(\"18 to 24\", \"25 to 29\", \"30 to 34\", \"35 to 39\", \"40 to 44\", \n                                 \"45 to 49\", \"50 to 54\", \"55 to 59\", \"60 to 64\", \"65 to 69\", \n                                 \"70 to 74\", \"75 to 79\", \"80 and over\")),\n         Education = factor(Education,\n                            levels = 1:6,\n                            labels = c(\"Never attended/Only Kindergarten\", \"Elementary\", \n                                       \"Some high school\", \"High school graduate\", \n                                       \"Some college/tech school\", \"College graduate\")),\n         Income = factor(Income,\n                         levels = 1:8,\n                         labels = c(\"Less than $10,000\", \"$10,000 to $14,999\", \n                                    \"$15,000 to $19,999\", \"$20,000 to $24,999\", \n                                    \"$25,000 to $34,999\", \"$35,000 to $49,999\",\n                                    \"$50,000 to $74,999\", \"$75,000 or greater\"))\n         )\n\nMake sure the factors that had coded values changed to descriptive ones look right\n\ndiabetes |&gt;\n  group_by(Sex) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  Sex     count\n  &lt;fct&gt;   &lt;int&gt;\n1 Female 141974\n2 Male   111706\n\n\n\ndiabetes |&gt;\n  group_by(Age) |&gt;\n  summarize(count = n())\n\n# A tibble: 13 × 2\n   Age         count\n   &lt;fct&gt;       &lt;int&gt;\n 1 18 to 24     5700\n 2 25 to 29     7598\n 3 30 to 34    11123\n 4 35 to 39    13823\n 5 40 to 44    16157\n 6 45 to 49    19819\n 7 50 to 54    26314\n 8 55 to 59    30832\n 9 60 to 64    33244\n10 65 to 69    32194\n11 70 to 74    23533\n12 75 to 79    15980\n13 80 and over 17363\n\n\n\ndiabetes |&gt;\n  group_by(GenHlth) |&gt;\n  summarize(count = n())\n\n# A tibble: 5 × 2\n  GenHlth   count\n  &lt;fct&gt;     &lt;int&gt;\n1 Excellent 45299\n2 Very Good 89084\n3 Good      75646\n4 Fair      31570\n5 Poor      12081\n\n\n\ndiabetes |&gt;\n  group_by(Education) |&gt;\n  summarize(count = n())\n\n# A tibble: 6 × 2\n  Education                         count\n  &lt;fct&gt;                             &lt;int&gt;\n1 Never attended/Only Kindergarten    174\n2 Elementary                         4043\n3 Some high school                   9478\n4 High school graduate              62750\n5 Some college/tech school          69910\n6 College graduate                 107325\n\n\n\ndiabetes |&gt;\n  group_by(Income) |&gt;\n  summarize(count = n())\n\n# A tibble: 8 × 2\n  Income             count\n  &lt;fct&gt;              &lt;int&gt;\n1 Less than $10,000   9811\n2 $10,000 to $14,999 11783\n3 $15,000 to $19,999 15994\n4 $20,000 to $24,999 20135\n5 $25,000 to $34,999 25883\n6 $35,000 to $49,999 36470\n7 $50,000 to $74,999 43219\n8 $75,000 or greater 90385\n\n\nCheck for missing values\n\ncolSums(is.na(diabetes))\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n\n\n\nSummary Statistics\nGoing to look at the five number summary for BMI\n\ndiabetes |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarize(min = min(BMI),\n            q1 = quantile(BMI, probs = 0.25),\n            median = median(BMI),\n            q3 = quantile(BMI, probs = 0.75),\n            max = max(BMI))\n\n# A tibble: 2 × 6\n  Diabetes_binary   min    q1 median    q3   max\n  &lt;fct&gt;           &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0                  12    24     27    31    98\n2 1                  13    27     31    35    98\n\n\nThis looks reasonable, but I’ll still do a density plot to make sure. Mainly to make sure that the values in the lower 25th and the upper 75th percentiles aren’t all grouped at the extremes, but instead spread out a bit. If everything above 35 or so are all 98’s for instance, I’d be concerned that 98 is actually a coded value for something rather than a true value (e.g. what if the true values are capped at 35, and anything exceeding it was coded as 98). If they’re spread out, then I’d be more comfortable assuming they’re valid outliers.\n\ndiabetes |&gt;\n  group_by(Age, Diabetes_binary) |&gt;\n  summarize(across(where(is.numeric),\n            mean,\n            .names = \"mean_{col}\"))\n\n`summarise()` has grouped output by 'Age'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 26 × 5\n# Groups:   Age [13]\n   Age      Diabetes_binary mean_BMI mean_MentHlth mean_PhysHlth\n   &lt;fct&gt;    &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1 18 to 24 0                   26.0          4.36          1.92\n 2 18 to 24 1                   29.7          5.68          6.46\n 3 25 to 29 0                   27.5          3.74          2.05\n 4 25 to 29 1                   34.2          6.93          6.14\n 5 30 to 34 0                   28.3          3.76          2.36\n 6 30 to 34 1                   34.2          6.16          6.06\n 7 35 to 39 0                   28.3          3.61          2.54\n 8 35 to 39 1                   34.4          6.87          6.24\n 9 40 to 44 0                   28.5          3.53          2.78\n10 40 to 44 1                   35.1          6.87          7.78\n# ℹ 16 more rows\n\n\n\nCorrelation to Diabetes Status\n\n# see if there are correlations between Diabetes status and all other columns\ncorrelation &lt;- diabetes |&gt;\n  lapply(as.numeric) |&gt;\n  as.data.frame() |&gt;\n  summarise(across(everything(), \n                   \\(x) list(cor.test(x, as.numeric(diabetes$Diabetes_binary))))) \n\n# transform the correlation statistics from list-column to a flat numeric column\ncorr_col &lt;- correlation |&gt;\n  map(1) |&gt;                 # access first element (a list) of each list-column\n  map(\"estimate\") |&gt;        # pull just the estimate element for each inner list\n  bind_rows()               # compress into a simple column\n\ntibble(Variable = names(correlation)) |&gt;\n  bind_cols(corr_col) |&gt;\n  filter(abs(cor) &gt; 0.02, cor &lt; 1.0) |&gt;\n  arrange(desc(abs(cor)))\n\n# A tibble: 20 × 2\n   Variable                 cor\n   &lt;chr&gt;                  &lt;dbl&gt;\n 1 GenHlth               0.294 \n 2 HighBP                0.263 \n 3 DiffWalk              0.218 \n 4 BMI                   0.217 \n 5 HighChol              0.200 \n 6 Age                   0.177 \n 7 HeartDiseaseorAttack  0.177 \n 8 PhysHlth              0.171 \n 9 Income               -0.164 \n10 Education            -0.124 \n11 PhysActivity         -0.118 \n12 Stroke                0.106 \n13 MentHlth              0.0693\n14 CholCheck             0.0648\n15 Smoker                0.0608\n16 HvyAlcoholConsump    -0.0571\n17 Veggies              -0.0566\n18 Fruits               -0.0408\n19 NoDocbcCost           0.0314\n20 Sex                   0.0314\n\n\nMight be good to focus on the top 5 variables in the above list as predictors. These variables are consistent with the fact that diabetes is associated with heart disease, as discussed in the documentation for the dataset. Difficulty walking may be related to neuropathy, which is also a complication of diabetes. Problems with circulation also occur in diabetics, and can also impact general health by reducing the effectiveness of the immune response (immune cells will have a harder time reaching sites of infection and damage if there are issues with blood flow). Therefore having some relationship between GenHlth and diabetes status is reasonable.\nI’ll look at the correlations between those 5 predictors and all other variables to see if they have associations with other variables that might also be interesting to look at.\n\ncorrelation_2 &lt;- diabetes |&gt;\n  map(as.numeric) |&gt;\n  as_tibble() |&gt;\n  cor()\n\n# just look at top 5\ncorrelation_2[,c(\"GenHlth\", \"HighBP\", \"DiffWalk\", \"BMI\", \"HighChol\")]\n\n                          GenHlth       HighBP     DiffWalk         BMI\nDiabetes_binary       0.293569063  0.263128790  0.218344352  0.21684306\nHighBP                0.300529631  1.000000000  0.223618466  0.21374812\nHighChol              0.208425550  0.298199295  0.144671538  0.10672208\nCholCheck             0.046588865  0.098508273  0.040585057  0.03449509\nBMI                   0.239185373  0.213748120  0.197077760  1.00000000\nSmoker                0.163143067  0.096991467  0.122463215  0.01380447\nStroke                0.177942260  0.129574913  0.176566917  0.02015266\nHeartDiseaseorAttack  0.258383409  0.209361211  0.212708695  0.05290426\nPhysActivity         -0.266185624 -0.125266866 -0.253174007 -0.14729363\nFruits               -0.103854171 -0.040554659 -0.048351675 -0.08751812\nVeggies              -0.123066330 -0.061266165 -0.080505717 -0.06227519\nHvyAlcoholConsump    -0.036723570 -0.003971574 -0.037668174 -0.04873628\nAnyHealthcare        -0.040817072  0.038424769  0.007074092 -0.01847079\nNoDocbcCost           0.166397186  0.017357984  0.118446862  0.05820629\nGenHlth               1.000000000  0.300529631  0.456919503  0.23918537\nMentHlth              0.301674393  0.056455917  0.233688079  0.08531016\nPhysHlth              0.524363644  0.161211571  0.478416619  0.12114111\nDiffWalk              0.456919503  0.223618466  1.000000000  0.19707776\nSex                  -0.006091004  0.052206961 -0.070298902  0.04295030\nAge                   0.152449830  0.344452330  0.204450090 -0.03661764\nEducation            -0.284911532 -0.141357934 -0.192642100 -0.10393202\nIncome               -0.370013734 -0.171234581 -0.320124244 -0.10006871\n                        HighChol\nDiabetes_binary       0.20027619\nHighBP                0.29819930\nHighChol              1.00000000\nCholCheck             0.08564223\nBMI                   0.10672208\nSmoker                0.09129936\nStroke                0.09262007\nHeartDiseaseorAttack  0.18076535\nPhysActivity         -0.07804619\nFruits               -0.04085908\nVeggies              -0.03987361\nHvyAlcoholConsump    -0.01154252\nAnyHealthcare         0.04222986\nNoDocbcCost           0.01331016\nGenHlth               0.20842555\nMentHlth              0.06206915\nPhysHlth              0.12175053\nDiffWalk              0.14467154\nSex                   0.03120533\nAge                   0.27231823\nEducation            -0.07080189\nIncome               -0.08545931\n\n\nNothing particularly highly correlated, just some weak associations between those “top 5” variables already identified, plus some association with GenHlth and DiffWalk/PhysHlth/Ment, and also some very weak correlation with heart attack history and the “top 5”.\n\n\nContingency Tables\nUsing the above correlations as a guide, I will look at contingency tables of the correlation values that are the farthest from 0 (although none are particularly high).\n\ndiabetes |&gt;\n  group_by(GenHlth, Diabetes_binary) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Diabetes_binary, \n              values_from = count, \n              names_prefix = \"Diabetes_\") |&gt;\n  mutate(total = Diabetes_0 + Diabetes_1,\n         Diabetes_0_pct = Diabetes_0 / total,\n         Diabetes_1_pct = Diabetes_1 / total)\n\n`summarise()` has grouped output by 'GenHlth'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 5 × 6\n# Groups:   GenHlth [5]\n  GenHlth   Diabetes_0 Diabetes_1 total Diabetes_0_pct Diabetes_1_pct\n  &lt;fct&gt;          &lt;int&gt;      &lt;int&gt; &lt;int&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1 Excellent      44159       1140 45299          0.975         0.0252\n2 Very Good      82703       6381 89084          0.928         0.0716\n3 Good           62189      13457 75646          0.822         0.178 \n4 Fair           21780       9790 31570          0.690         0.310 \n5 Poor            7503       4578 12081          0.621         0.379 \n\n\n\ndiabetes |&gt;\n  group_by(HighBP, Diabetes_binary) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Diabetes_binary, \n              values_from = count, \n              names_prefix = \"Diabetes_\") |&gt;\n  mutate(total = Diabetes_0 + Diabetes_1,\n         Diabetes_0_pct = Diabetes_0 / total,\n         Diabetes_1_pct = Diabetes_1 / total)\n\n`summarise()` has grouped output by 'HighBP'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 6\n# Groups:   HighBP [2]\n  HighBP Diabetes_0 Diabetes_1  total Diabetes_0_pct Diabetes_1_pct\n  &lt;fct&gt;       &lt;int&gt;      &lt;int&gt;  &lt;int&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1 0          136109       8742 144851          0.940         0.0604\n2 1           82225      26604 108829          0.756         0.244 \n\n\n\ndiabetes |&gt;\n  group_by(DiffWalk, Diabetes_binary) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Diabetes_binary, \n              values_from = count, \n              names_prefix = \"Diabetes_\") |&gt;\n  mutate(total = Diabetes_0 + Diabetes_1,\n         Diabetes_0_pct = Diabetes_0 / total,\n         Diabetes_1_pct = Diabetes_1 / total)\n\n`summarise()` has grouped output by 'DiffWalk'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 6\n# Groups:   DiffWalk [2]\n  DiffWalk Diabetes_0 Diabetes_1  total Diabetes_0_pct Diabetes_1_pct\n  &lt;fct&gt;         &lt;int&gt;      &lt;int&gt;  &lt;int&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1 0            188780      22225 211005          0.895          0.105\n2 1             29554      13121  42675          0.693          0.307\n\n\n\ndiabetes |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarize(count = n(),\n            mean_BMI = mean(BMI))\n\n# A tibble: 2 × 3\n  Diabetes_binary  count mean_BMI\n  &lt;fct&gt;            &lt;int&gt;    &lt;dbl&gt;\n1 0               218334     27.8\n2 1                35346     31.9\n\n\n\ndiabetes |&gt;\n  group_by(HighChol, Diabetes_binary) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Diabetes_binary, \n              values_from = count, \n              names_prefix = \"Diabetes_\") |&gt;\n  mutate(total = Diabetes_0 + Diabetes_1,\n         Diabetes_0_pct = Diabetes_0 / total,\n         Diabetes_1_pct = Diabetes_1 / total)\n\n`summarise()` has grouped output by 'HighChol'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 6\n# Groups:   HighChol [2]\n  HighChol Diabetes_0 Diabetes_1  total Diabetes_0_pct Diabetes_1_pct\n  &lt;fct&gt;         &lt;int&gt;      &lt;int&gt;  &lt;int&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1 0            134429      11660 146089          0.920         0.0798\n2 1             83905      23686 107591          0.780         0.220 \n\n\n\n\n\nPlots\n\nBar Chart\nHere is a bar chart showing how a higher proportion of respondents reporting Good, Fair, or Poor health have a higher rate of diabetes/prediabetes than respondents reporting Very Good or Excellent health.\n\ng &lt;- ggplot(data = diabetes, aes(x = GenHlth))\ng + geom_bar(aes(fill = Diabetes_binary)) \n\n\n\n\n\n\n\n\nBecause there was some correlation between the diabetes status and GenHlth, and there was also correlation between GenHlth and the days of physical and/or mental health difficulties, I thought it would be cool to put them on a contour map to see if there were patterns for diabetes status vs Physical and Mental health days.\nWhen I initially plotted the original data points for physical and mental health days, which had a range of 0-30, there were many spots on the plot that were blank because there were no respondents that gave that particular combination of days. I set up a rounded column for each of these variables which rounded the number down to the nearest 5 (so 0-4 = 0, 5-9 = 5, 10-14 = 10, etc.).\nI also added in a new column just for the plot which gives the proportion in each of those combinations had diabetes or prediabetes. This was used as the z-value for the plot.\n\n# note: Diabetes_binary is converted to character first since it's a factor, and I want to make sure to sum up the original values of 0 and 1 and not the factor levels.\ndiabetes_rollup &lt;- diabetes |&gt;\n  mutate(PhysHlth_rnd = floor(PhysHlth/5)*5,\n         MentHlth_rnd = floor(MentHlth/5)*5) |&gt;\n  group_by(PhysHlth_rnd, MentHlth_rnd) |&gt;\n  summarise(Diabetes_prop = sum(as.numeric(as.character(Diabetes_binary)))/n())\n\n`summarise()` has grouped output by 'PhysHlth_rnd'. You can override using the\n`.groups` argument.\n\nh &lt;- ggplot(data = diabetes_rollup, aes(x = PhysHlth_rnd, y = MentHlth_rnd)) \nh + geom_contour_filled(aes(z = Diabetes_prop)) +\n  labs(x = \"Days with Physical Health Difficulties\",\n       y = \"Days with Mental Health Difficulties\",\n       fill = \"Proportion \\nwith diabetes/ \\nprediabetes\") \n\n\n\n\n\n\n\n\nThis shows a cool pattern where the more days that a person reported sick or injured days, the higher the proportion of diabetes/prediabetes. This effect looks more related to physical health issues when there are fewer than 10-15 days with those types of issues, but as the number of days gets higher, the people with more days of mental health struggles start to have higher rates of diabetes/prediabetes. Anecdotally, this seems consistent with the assumption that a person with these conditions who reports many days of the month with physical health burdens may start to struggle with mental health as well.\n\n\nBoxplot\nBoxplot for BMI to see the distribution of values\n\ng &lt;- ggplot(data = diabetes, aes(x = Diabetes_binary, y = BMI, fill = Diabetes_binary))\n\ng + geom_boxplot()\n\n\n\n\n\n\n\n\nIt’s interesting to note that the median BMI is higher in the Has Prediabetes/Diabetes group than the other. But both groups have a large number of BMI’s outside the middle 50%, which is interesting.\n\n\nDensity Plot\n\ng &lt;- ggplot(data = diabetes, aes(x = BMI, fill = Diabetes_binary))\n\ng + geom_density(alpha = 0.5) +\n  geom_vline(aes(xintercept = median(BMI[Diabetes_binary == 0])), \n             color = \"darkgreen\", \n             linetype = \"dashed\") +\n  geom_vline(aes(xintercept = median(BMI[Diabetes_binary == 1])), \n             color = \"navy\", \n             linetype = \"dashed\") +\n  scale_fill_manual(values = c(\"paleturquoise\", \"peachpuff\")) +\n  labs(x = \"BMI\", y = \"Proportion\")\n\n\n\n\n\n\n\n\nDensity plot provides another view showing that the prediabetes/diabetes group has higher BMI on average, and we can also see that although there’s a long tail on the right side, there aren’t a huge number of points in the extremes relative to the size of the data set.\n\n\n\nTo the Models\nWith the EDA complete, and predictors selected, we can now fit the models.\nGo to the Modeling Page"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "Using a subset of variables from the Diabetes Health Indicators Dataset, two types of tree-based models will now be trained on a training set using 5-fold CV. Training data will be used to assess the log-loss of a classification tree model and a random forest model. The best parameters from each will be selected and then fit on the test set to determine which model is best. That final model will then be fit to the entire dataset, and an API will be set up with three endpoints. The first endpoint will return a prediction of diabetes status based on the input parameters; the second endpoint will provide my name and the github URL; and the third will show a plot of the confusion matrix for the final model."
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "Modeling",
    "section": "",
    "text": "Using a subset of variables from the Diabetes Health Indicators Dataset, two types of tree-based models will now be trained on a training set using 5-fold CV. Training data will be used to assess the log-loss of a classification tree model and a random forest model. The best parameters from each will be selected and then fit on the test set to determine which model is best. That final model will then be fit to the entire dataset, and an API will be set up with three endpoints. The first endpoint will return a prediction of diabetes status based on the input parameters; the second endpoint will provide my name and the github URL; and the third will show a plot of the confusion matrix for the final model."
  },
  {
    "objectID": "Modeling.html#setup",
    "href": "Modeling.html#setup",
    "title": "Modeling",
    "section": "Setup",
    "text": "Setup\nData needs to be split, with 70% training and 30% testing\n\nset.seed(42)\n\ndiabetes_split &lt;- initial_split(diabetes, prop = 0.7)\ndiabetes_train &lt;- training(diabetes_split)\ndiabetes_test &lt;- testing(diabetes_split)\n\nSet up CV folds\n\ndiabetes_CV_folds &lt;- vfold_cv(diabetes_train, 5)\n\n\nRecipe\nNumeric predictors will be standardized, and dummy variables will be created for the binary and categorial variables.\n\ntree_rec &lt;- recipe(Diabetes_binary ~ GenHlth + HighBP + HighChol + DiffWalk + \n                     BMI + PhysHlth,\n                   data = diabetes_train) |&gt;\n  step_normalize(all_numeric()) |&gt;\n  step_dummy(GenHlth, HighBP, HighChol, DiffWalk)\n\ntree_rec |&gt;\n  prep(diabetes_train) |&gt;\n  bake(diabetes_train)\n\n# A tibble: 177,576 × 10\n       BMI PhysHlth Diabetes_binary GenHlth_Very.Good GenHlth_Good GenHlth_Fair\n     &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;                       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1 -0.209    -0.486 0                               1            0            0\n 2 -0.209    -0.486 0                               1            0            0\n 3 -0.510    -0.486 0                               0            0            0\n 4  1.15     -0.371 0                               1            0            0\n 5  1.75     -0.486 0                               0            0            1\n 6 -0.661    -0.486 0                               0            0            0\n 7 -0.661    -0.486 0                               0            0            0\n 8  0.546     2.96  0                               0            0            1\n 9 -0.0578   -0.486 0                               0            1            0\n10  1.45     -0.256 0                               0            1            0\n# ℹ 177,566 more rows\n# ℹ 4 more variables: GenHlth_Poor &lt;dbl&gt;, HighBP_X1 &lt;dbl&gt;, HighChol_X1 &lt;dbl&gt;,\n#   DiffWalk_X1 &lt;dbl&gt;"
  },
  {
    "objectID": "Modeling.html#classification-tree",
    "href": "Modeling.html#classification-tree",
    "title": "Modeling",
    "section": "Classification Tree",
    "text": "Classification Tree\nA classification tree is a supervised learning model that attempts to predict the value of a categorical response variable based on the values of one or more predictor variables. It does this by partitioning the predictor space into different branches, ultimately arriving at a prediction for the response variable. These branches are created using recursive binary splitting, which determines the optimal split for the current node only. The best split is determined by minimizing the squared error loss evaluated for each potential value of the predictors. Since it does not consider potential future splits, this can result in the tree missing out on a better match down the line that could have led to more accurate predictions. However, the classification tree is easy to understand, and is an effective and flexible model for situations where the data does not conform well to a line or plane.\n\nSet up Model and Workflow\nModel for classification tree, with tuning parameters\n\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 10,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\nWorkflow\n\ntree_wkf &lt;- workflow() |&gt;\n  add_recipe(tree_rec) |&gt;\n  add_model(tree_mod)\n\n\n\nTuning parameters\nSet up a tuning grid\n\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(10, 5))\n\nRun tuning on the specified grid\n\ntree_fits &lt;- tree_wkf |&gt;\n  tune_grid(resamples = diabetes_CV_folds,\n            grid = tree_grid,\n            metrics = metric_set(accuracy, mn_log_loss))\n\nGet the best parameters\n\ntree_best_params &lt;- select_best(tree_fits, metric = \"mn_log_loss\")\n\ntree_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001          8 Preprocessor1_Model21\n\n\nRefit on the training set\n\ntree_final_fit &lt;- tree_wkf |&gt;\n  finalize_workflow(tree_best_params) |&gt;\n  last_fit(diabetes_split, metrics = metric_set(accuracy, mn_log_loss))\n\ntree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.865 Preprocessor1_Model1\n2 mn_log_loss binary         0.342 Preprocessor1_Model1\n\n\n\n\nFinal Model\n\ntree_final_model &lt;- extract_workflow(tree_final_fit)\n\ntree_final_model |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot::rpart.plot(roundint = FALSE)\n\nWarning: labs do not fit even at cex 0.15, there may be some overplotting"
  },
  {
    "objectID": "Modeling.html#random-forest",
    "href": "Modeling.html#random-forest",
    "title": "Modeling",
    "section": "Random Forest",
    "text": "Random Forest\nA random forest model is a supervised learning ensemble method that utilizes bootstrapping to generate many single trees to predict a response variable based on the values of one or more predictor variables. Bootstrapping is a method of resampling the data with replacement; in a random forest model, those samples are then used to fit many trees, and the best prediction across all trees is chosen. For regression trees, the best prediction is the average of all predictions, and for classification trees, the best prediction is the most common one. In a random forest model, only a random subset of predictors is considered for each split of a single tree. This makes the different trees less correlated with each other, which has the potential to improve the overall accuracy of the prediction by reducing the variance between the predictions of the single trees.\n\nSet up Model and Workflow\n\n# model specifications\nrf_model &lt;- rand_forest(mtry = tune()) |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"classification\")\n\n# workflow using same recipe already set up\nrf_wkf &lt;- workflow() |&gt;\n  add_recipe(tree_rec) |&gt;\n  add_model(rf_model)\n\n\n\nTuning parameters\nTune on a grid with 10 values\n\nrf_fit &lt;- rf_wkf |&gt;\n  tune_grid(resamples = diabetes_CV_folds,\n            grid = 10,\n            metrics = metric_set(accuracy, mn_log_loss))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWarning: package 'ranger' was built under R version 4.4.2\n\n\nSelect best parameters\n\nrf_best_params &lt;- rf_fit |&gt;\n  select_best(metric = \"mn_log_loss\")\n\nrf_best_params\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1     4 Preprocessor1_Model9\n\n\n\n\nFinal Model\nFinalize the workflow\n\nrf_final_fit &lt;- rf_wkf |&gt;\n  finalize_workflow(rf_best_params) |&gt;\n  last_fit(diabetes_split, metrics = metric_set(accuracy, mn_log_loss))\n\nWarning: package 'ranger' was built under R version 4.4.2"
  },
  {
    "objectID": "Modeling.html#compare-models",
    "href": "Modeling.html#compare-models",
    "title": "Modeling",
    "section": "Compare Models",
    "text": "Compare Models\nCompare the log loss for both models\n\nrf_metrics &lt;- rf_final_fit |&gt;\n  collect_metrics() |&gt;\n  bind_cols(method = rep(\"Random Forest\", 2)) \n\ntree_metrics &lt;- tree_final_fit |&gt;\n  collect_metrics() |&gt;\n  bind_cols(method = rep(\"Classification Tree\", 2)) \n\nrf_metrics |&gt;\n  bind_rows(tree_metrics)\n\n# A tibble: 4 × 5\n  .metric     .estimator .estimate .config              method             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt;              \n1 accuracy    binary         0.865 Preprocessor1_Model1 Random Forest      \n2 mn_log_loss binary         0.325 Preprocessor1_Model1 Random Forest      \n3 accuracy    binary         0.865 Preprocessor1_Model1 Classification Tree\n4 mn_log_loss binary         0.342 Preprocessor1_Model1 Classification Tree\n\n\nThe accuracy of each model is roughly the same, but Random Forest has a marginally better log loss, so this one is the overall winner."
  },
  {
    "objectID": "Modeling.html#fit-best-model-to-entire-data",
    "href": "Modeling.html#fit-best-model-to-entire-data",
    "title": "Modeling",
    "section": "Fit Best Model to Entire Data",
    "text": "Fit Best Model to Entire Data\nFit the best model to the entire data set, for use by the API.\n\n# model specifications (using the optimal mtry value found during tuning)\nrf_model &lt;- rand_forest(mtry = 4) |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"classification\")\n\n# workflow \nrf_wkf &lt;- workflow() |&gt;\n  add_recipe(tree_rec) |&gt;\n  add_model(rf_model)\n\n# fit best model on entire data set\nrf_fit_full &lt;- rf_wkf |&gt;\n  fit(diabetes)\n\n# save model for later use\nsaveRDS(rf_fit_full, file = \"rf_fit_model.rda\")\n\nAlso saved the confusion matrix, since it’s a tad long-running\n\n# save the confusion matrix\nrf_conf_mat &lt;- conf_mat(diabetes |&gt; mutate(estimate = rf_fit_model |&gt; \n                                             predict(diabetes) |&gt; pull()),\n                        Diabetes_binary,\n                        estimate)\n\nsaveRDS(rf_conf_mat, file = \"rf_conf_matrix.rda\")"
  }
]